{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEED_VAL = 200;\n",
    "WORK_DIR = os.getcwd();\n",
    "YELP_DATA_CSV_DIR = os.path.join(WORK_DIR, \"data\", \"csv\")\n",
    "YELP_DATA_WORD_2_VEC_MODEL_DIR = os.path.join(WORK_DIR, \"data\", \"word2vec_model\")\n",
    "YELP_DATA_FEAT_ADD = os.path.join(WORK_DIR, \"data\", \"pos_neg\")\n",
    "YELP_DATA_WORD_2_VEC_MODEL_DIR_FEAT_ADD = os.path.join(WORK_DIR, \"data\", \"word2vec_model_feat_add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_sure_path_exists(YELP_DATA_WORD_2_VEC_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py:1170: DtypeWarning: Columns (5,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n"
     ]
    }
   ],
   "source": [
    "read_filename = os.path.join(YELP_DATA_CSV_DIR, 'business_review_user_10Percent.csv')\n",
    "df_data = pd.read_csv(read_filename, engine='c', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Great outdoor patio dining area. Great happy hour. Great service.\\r\\n\\r\\nOutdoor patio dining has a beautiful mesquite tree for ambiance and blocking out the sun while the center fireplace keeps you warm. \\r\\n\\r\\nWe had:\\r\\nQueso Skillet with warm tortilla chips - amazing!\\r\\nMac N Cheese with Chorizo - fabulous! one of the best mac n cheeses I've ever had!\\r\\nCarne Asada on a Potato Pancake - was ok. Sounded excellent, tasted decent.\\r\\n\\r\\nFriendly and good food. But the ambiance really puts it over the top as a great dining experience. I'd be back with a group of friends to lounge, play cornsack or bocce ball during happy hour.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.review_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem(s):\n",
    "    s = remove_numbers_in_string(s)\n",
    "    s = lowercase_remove_punctuation(s)\n",
    "    s = remove_stopwords(s)\n",
    "    token_list = nltk.word_tokenize(s)\n",
    "    #token_list = filter_out_more_stopwords(token_list)\n",
    "    token_list = stem_token_list(token_list)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'great', u'outdoor', u'patio', u'dine', u'area', u'great', u'happi', u'hour', u'great', u'servic', u'outdoor', u'patio', u'dine', u'beauti', u'mesquit', u'tree', u'ambianc', u'block', u'sun', u'center', u'fireplac', u'keep', u'warm', u'queso', u'skillet', u'warm', u'tortilla', u'chip', u'amaz', u'mac', u'n', u'chees', u'chorizo', u'fabul', u'one', u'best', u'mac', u'n', u'chees', u'ive', u'ever', u'carn', u'asada', u'potato', u'pancak', u'ok', u'sound', u'excel', u'tast', u'decent', u'friendli', u'good', u'food', u'ambianc', u'realli', u'put', u'top', u'great', u'dine', u'experi', u'id', u'back', u'group', u'friend', u'loung', u'play', u'cornsack', u'bocc', u'ball', u'happi', u'hour']\n"
     ]
    }
   ],
   "source": [
    "print lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem(df_data.review_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem( raw_sentence))\n",
    "    \n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in df_data[\"review_text\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'locat', u'chevron', u'ga', u'station', u'lot']\n"
     ]
    }
   ],
   "source": [
    "print sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data['pos_words'] = ''\n",
    "df_data['neg_words'] = ''\n",
    "df_data['net_senti'] = ''\n",
    "df_data['review_senti'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POS_WORDS_FILE = os.path.join(YELP_DATA_FEAT_ADD, 'positive-words.txt')\n",
    "NEG_WORDS_FILE = os.path.join(YELP_DATA_FEAT_ADD, 'negative-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 positive words ['a+', 'abound', 'abounds', 'abundance', 'abundant'] \n",
      "First 5 negative words ['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable']\n",
      "Number of positive words 2006\n",
      "Number of negative words 4783\n",
      "Total number of words 6789\n"
     ]
    }
   ],
   "source": [
    "with open(POS_WORDS_FILE) as f:\n",
    "    pos_words1 = f.read().split()[213:]\n",
    "\n",
    "with open(NEG_WORDS_FILE) as f:\n",
    "    neg_words1 = f.read().split()[213:]    \n",
    "    \n",
    "print \"First 5 positive words %s \" % pos_words1[:5]\n",
    "print \"First 5 negative words %s\" % neg_words1[:5]\n",
    "\n",
    "print \"Number of positive words %d\" % len(pos_words1)\n",
    "\n",
    "print \"Number of negative words %d\" % len(neg_words1)\n",
    "\n",
    "all_words_with_sentiment = pos_words1 + neg_words1\n",
    "\n",
    "print \"Total number of words %d\" % len(all_words_with_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords(s):\n",
    "    s = remove_numbers_in_string(s)\n",
    "    s = lowercase_remove_punctuation(s)\n",
    "    s = remove_stopwords(s)\n",
    "    token_list = []\n",
    "    token_list.append(nltk.word_tokenize(s))\n",
    "    #token_list = filter_out_more_stopwords(token_list)\n",
    "    #token_list = stem_token_list(token_list)\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n",
      "219285\n",
      "219285\n",
      "[u'located', u'chevron', u'gas', u'station', u'lot', u'quick', u'easy', u'done', u'less', u'minutes', u'smog', u'guy', u'pleasant', u'enough', u'end']\n"
     ]
    }
   ],
   "source": [
    "all_reviews = [] # Initialize an empty list of sentences for each review\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in df_data[\"review_text\"]:\n",
    "    all_reviews += lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords(review)\n",
    "\n",
    "print len(all_reviews)\n",
    "print len(df_data)\n",
    "print all_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:4: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for sentence in all_reviews:\n",
    "    num_positive = sum([r in pos_words1 for r in sentence])\n",
    "    num_negative = sum([r in neg_words1 for r in sentence])\n",
    "    net_sentiment = num_positive - num_negative\n",
    "     \n",
    "#   num_positive #adds number of positive words to the dataframe for each review\n",
    "#   num_negative #adds number of negative words to the dataframe for each review\n",
    "#   net_sentiment #adds total weight to the dataframe for each review\n",
    "    \n",
    "    \n",
    "    df_data.set_value(i, 'pos_words', num_positive)\n",
    "    df_data.set_value(i, 'neg_words', num_negative)\n",
    "    df_data.set_value(i, 'net_senti', net_sentiment)\n",
    "    \n",
    "\n",
    "    if(net_sentiment > 0):\n",
    "        #adds 1 to the dataframe for each review denoting a positive tendency of the review\n",
    "        df_data.set_value(i, 'review_senti', 1)\n",
    "    elif(net_sentiment < 0):\n",
    "        #adds -1 to the dataframe for each review denoting a negative tendency of the review\n",
    "         df_data.set_value(i, 'review_senti', -1)\n",
    "    else:\n",
    "        #adds 0 to the dataframe for each review denoting a neutral tendency of the review\n",
    "         df_data.set_value(i, 'review_senti', 0)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              business_id                                business_categories  \\\n",
      "0  FoHJy_ucYarA2DkCrAVIgw  ['Automotive', 'Smog Check Stations', 'Registr...   \n",
      "1  FqzgT9Y-Yu7jiWdHnGW-kQ  ['Bars', 'American (Traditional)', 'Pubs', 'Ni...   \n",
      "2  n5eQnMnVVt3FfrFENYoU0g       ['Latin American', 'Mexican', 'Restaurants']   \n",
      "3  E2XPkjbbhdNY2yXBTwScQw  ['Gluten-Free', 'Sandwiches', 'Salad', 'Restau...   \n",
      "4  hfUdBRgTTPTR4s4MOqux8Q      ['Arts & Entertainment', 'Stadiums & Arenas']   \n",
      "5  aKP2JIVNqVtXLkSEMHKRsw            ['Fast Food', 'Chinese', 'Restaurants']   \n",
      "6  OnojjpVrKB02aualscTnmA       ['Car Wash', 'Automotive', 'Auto Detailing']   \n",
      "7  yYbd9P1KmlPSKmQxo68n_g  ['Caterers', 'Event Planning & Services', 'Bar...   \n",
      "8  PlcCjELzSI3SqX7mPF5cCw  ['Seafood', 'Sushi Bars', 'Japanese', 'Restaur...   \n",
      "9  tXydHtnQ2sU37kXy30CBLw  ['Bars', 'Tapas/Small Plates', 'Wine Bars', 'N...   \n",
      "\n",
      "                        business_name  business_review_count  \\\n",
      "0                          Jiffy Smog                     11   \n",
      "1                             The Vig                    522   \n",
      "2                      Cabo Fish Taco                    672   \n",
      "3                  Muscle Maker Grill                     31   \n",
      "4                Thomas & Mack Center                     96   \n",
      "5                       Panda Express                     15   \n",
      "6                 Alamo Hand Car Wash                    266   \n",
      "7                      Pork On A Fork                    398   \n",
      "8            SakeBomber Sushi & Grill                    212   \n",
      "9  Double Helix Wine & Whiskey Lounge                    217   \n",
      "\n",
      "                               business_full_address business_open  \\\n",
      "0  3061 St Rose Pkwy\\r\\nSoutheast\\r\\nLas Vegas, N...          True   \n",
      "1                4041 N 40th St\\r\\nPhoenix, AZ 85018          True   \n",
      "2  3201 N Davidson St\\r\\nNoDa\\r\\nCharlotte, NC 28205          True   \n",
      "3  8386 W Thunderbird Rd\\r\\nSte 103\\r\\nPeoria, AZ...          True   \n",
      "4  Swenson & Thomas and Mack Dr\\r\\nUniversity\\r\\n...          True   \n",
      "5  7017 Spring Mountain Rd\\r\\nChinatown\\r\\nLas Ve...          True   \n",
      "6  5705 S Rainbow Blvd\\r\\nSpring Valley\\r\\nLas Ve...          True   \n",
      "7  1515 W Deer Valley Rd\\r\\nSte B-102\\r\\nPhoenix,...          True   \n",
      "8              1705 E Broadway Rd\\r\\nTempe, AZ 85282          True   \n",
      "9  Town Square\\r\\n6599 Las Vegas Blvd S ,Ste 150b...          True   \n",
      "\n",
      "   business_stars business_type  business_latitude business_longitude  \\\n",
      "0             2.5      business          35.999862          -115.1231   \n",
      "1             4.0      business          33.494489          -111.9951   \n",
      "2             3.5      business          35.247251          -80.80583   \n",
      "3             4.0      business          33.610449          -112.2394   \n",
      "4             4.0      business          36.104761          -115.1444   \n",
      "5             3.0      business          36.125315          -115.2464   \n",
      "6             4.0      business          36.084609          -115.2437   \n",
      "7             4.0      business          33.683323          -112.0924   \n",
      "8             3.5      business          33.407088          -111.9115   \n",
      "9             4.0      business          36.067842          -115.1746   \n",
      "\n",
      "       ...      user_type user_average_stars  \\\n",
      "0      ...           user               3.50   \n",
      "1      ...           user               3.68   \n",
      "2      ...           user               3.40   \n",
      "3      ...           user               3.76   \n",
      "4      ...           user               4.00   \n",
      "5      ...           user               4.78   \n",
      "6      ...           user               4.06   \n",
      "7      ...           user               3.05   \n",
      "8      ...           user               3.52   \n",
      "9      ...           user               2.82   \n",
      "\n",
      "                                   user_elite user_votes.cool  \\\n",
      "0                          [2013, 2014, 2015]             767   \n",
      "1  [2009, 2010, 2011, 2012, 2013, 2014, 2015]             622   \n",
      "2                                          []               0   \n",
      "3                                          []               7   \n",
      "4                    [2011, 2012, 2013, 2014]             180   \n",
      "5                                          []              15   \n",
      "6                                          []              12   \n",
      "7                                          []              50   \n",
      "8                                          []              18   \n",
      "9                                          []               6   \n",
      "\n",
      "   user_votes.funny  user_votes.useful  pos_words neg_words net_senti  \\\n",
      "0               781               1219          3         0         3   \n",
      "1               463               1227         17         0        17   \n",
      "2                 0                  3          0         0         0   \n",
      "3                 6                 24          7         2         5   \n",
      "4               207                414         18         5        13   \n",
      "5                 4                 22          3         0         3   \n",
      "6                 9                 27          5         0         5   \n",
      "7                96                205          3         2         1   \n",
      "8                21                101         23        14         9   \n",
      "9                 4                  7          2         3        -1   \n",
      "\n",
      "   review_senti  \n",
      "0             1  \n",
      "1             1  \n",
      "2             0  \n",
      "3             1  \n",
      "4             1  \n",
      "5             1  \n",
      "6             1  \n",
      "7             1  \n",
      "8             1  \n",
      "9            -1  \n",
      "\n",
      "[10 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "print df_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import gensim, logging\n",
    "from gensim.models import Word2Vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 8       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "model_file = os.path.join(YELP_DATA_WORD_2_VEC_MODEL_DIR, \"100features_10minwords_10context__10Percent\")\n",
    "print \"Training model...\"\n",
    "if not os.path.isfile(model_file):\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count, \\\n",
    "                window = context, sample = downsampling)\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "    model.save(model_file)\n",
    "else:\n",
    "    print \"Loading existing model\"\n",
    "    model = Word2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amazing'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"amazing delightful bad\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       \n",
    "       # Print a status message every 1000th review\n",
    "        if counter%1000. == 0.:\n",
    "               print \"Review %d of %d\" % (counter, len(reviews))\n",
    "       \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       \n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 219285\n",
      "Review 1000 of 219285\n",
      "Review 2000 of 219285\n",
      "Review 3000 of 219285\n",
      "Review 4000 of 219285\n",
      "Review 5000 of 219285\n",
      "Review 6000 of 219285\n",
      "Review 7000 of 219285\n",
      "Review 8000 of 219285\n",
      "Review 9000 of 219285\n",
      "Review 10000 of 219285\n",
      "Review 11000 of 219285\n",
      "Review 12000 of 219285\n",
      "Review 13000 of 219285\n",
      "Review 14000 of 219285\n",
      "Review 15000 of 219285\n",
      "Review 16000 of 219285\n",
      "Review 17000 of 219285\n",
      "Review 18000 of 219285\n",
      "Review 19000 of 219285\n",
      "Review 20000 of 219285\n",
      "Review 21000 of 219285\n",
      "Review 22000 of 219285\n",
      "Review 23000 of 219285\n",
      "Review 24000 of 219285\n",
      "Review 25000 of 219285\n",
      "Review 26000 of 219285\n",
      "Review 27000 of 219285\n",
      "Review 28000 of 219285\n",
      "Review 29000 of 219285\n",
      "Review 30000 of 219285\n",
      "Review 31000 of 219285\n",
      "Review 32000 of 219285\n",
      "Review 33000 of 219285\n",
      "Review 34000 of 219285\n",
      "Review 35000 of 219285\n",
      "Review 36000 of 219285\n",
      "Review 37000 of 219285\n",
      "Review 38000 of 219285\n",
      "Review 39000 of 219285\n",
      "Review 40000 of 219285\n",
      "Review 41000 of 219285\n",
      "Review 42000 of 219285\n",
      "Review 43000 of 219285\n",
      "Review 44000 of 219285\n",
      "Review 45000 of 219285\n",
      "Review 46000 of 219285\n",
      "Review 47000 of 219285\n",
      "Review 48000 of 219285\n",
      "Review 49000 of 219285\n",
      "Review 50000 of 219285\n",
      "Review 51000 of 219285\n",
      "Review 52000 of 219285\n",
      "Review 53000 of 219285\n",
      "Review 54000 of 219285\n",
      "Review 55000 of 219285\n",
      "Review 56000 of 219285\n",
      "Review 57000 of 219285\n",
      "Review 58000 of 219285\n",
      "Review 59000 of 219285\n",
      "Review 60000 of 219285\n",
      "Review 61000 of 219285\n",
      "Review 62000 of 219285\n",
      "Review 63000 of 219285\n",
      "Review 64000 of 219285\n",
      "Review 65000 of 219285\n",
      "Review 66000 of 219285\n",
      "Review 67000 of 219285\n",
      "Review 68000 of 219285\n",
      "Review 69000 of 219285\n",
      "Review 70000 of 219285\n",
      "Review 71000 of 219285\n",
      "Review 72000 of 219285\n",
      "Review 73000 of 219285\n",
      "Review 74000 of 219285\n",
      "Review 75000 of 219285\n",
      "Review 76000 of 219285\n",
      "Review 77000 of 219285\n",
      "Review 78000 of 219285\n",
      "Review 79000 of 219285\n",
      "Review 80000 of 219285\n",
      "Review 81000 of 219285\n",
      "Review 82000 of 219285\n",
      "Review 83000 of 219285\n",
      "Review 84000 of 219285\n",
      "Review 85000 of 219285\n",
      "Review 86000 of 219285\n",
      "Review 87000 of 219285\n",
      "Review 88000 of 219285\n",
      "Review 89000 of 219285\n",
      "Review 90000 of 219285\n",
      "Review 91000 of 219285\n",
      "Review 92000 of 219285\n",
      "Review 93000 of 219285\n",
      "Review 94000 of 219285\n",
      "Review 95000 of 219285\n",
      "Review 96000 of 219285\n",
      "Review 97000 of 219285\n",
      "Review 98000 of 219285\n",
      "Review 99000 of 219285\n",
      "Review 100000 of 219285\n",
      "Review 101000 of 219285\n",
      "Review 102000 of 219285\n",
      "Review 103000 of 219285\n",
      "Review 104000 of 219285\n",
      "Review 105000 of 219285\n",
      "Review 106000 of 219285\n",
      "Review 107000 of 219285\n",
      "Review 108000 of 219285\n",
      "Review 109000 of 219285\n",
      "Review 110000 of 219285\n",
      "Review 111000 of 219285\n",
      "Review 112000 of 219285\n",
      "Review 113000 of 219285\n",
      "Review 114000 of 219285\n",
      "Review 115000 of 219285\n",
      "Review 116000 of 219285\n",
      "Review 117000 of 219285\n",
      "Review 118000 of 219285\n",
      "Review 119000 of 219285\n",
      "Review 120000 of 219285\n",
      "Review 121000 of 219285\n",
      "Review 122000 of 219285\n",
      "Review 123000 of 219285\n",
      "Review 124000 of 219285\n",
      "Review 125000 of 219285\n",
      "Review 126000 of 219285\n",
      "Review 127000 of 219285\n",
      "Review 128000 of 219285\n",
      "Review 129000 of 219285\n",
      "Review 130000 of 219285\n",
      "Review 131000 of 219285\n",
      "Review 132000 of 219285\n",
      "Review 133000 of 219285\n",
      "Review 134000 of 219285\n",
      "Review 135000 of 219285\n",
      "Review 136000 of 219285\n",
      "Review 137000 of 219285\n",
      "Review 138000 of 219285\n",
      "Review 139000 of 219285\n",
      "Review 140000 of 219285\n",
      "Review 141000 of 219285\n",
      "Review 142000 of 219285\n",
      "Review 143000 of 219285\n",
      "Review 144000 of 219285\n",
      "Review 145000 of 219285\n",
      "Review 146000 of 219285\n",
      "Review 147000 of 219285\n",
      "Review 148000 of 219285\n",
      "Review 149000 of 219285\n",
      "Review 150000 of 219285\n",
      "Review 151000 of 219285\n",
      "Review 152000 of 219285\n",
      "Review 153000 of 219285\n",
      "Review 154000 of 219285\n",
      "Review 155000 of 219285\n",
      "Review 156000 of 219285\n",
      "Review 157000 of 219285\n",
      "Review 158000 of 219285\n",
      "Review 159000 of 219285\n",
      "Review 160000 of 219285\n",
      "Review 161000 of 219285\n",
      "Review 162000 of 219285\n",
      "Review 163000 of 219285\n",
      "Review 164000 of 219285\n",
      "Review 165000 of 219285\n",
      "Review 166000 of 219285\n",
      "Review 167000 of 219285\n",
      "Review 168000 of 219285\n",
      "Review 169000 of 219285\n",
      "Review 170000 of 219285\n",
      "Review 171000 of 219285\n",
      "Review 172000 of 219285\n",
      "Review 173000 of 219285\n",
      "Review 174000 of 219285\n",
      "Review 175000 of 219285\n",
      "Review 176000 of 219285\n",
      "Review 177000 of 219285\n",
      "Review 178000 of 219285\n",
      "Review 179000 of 219285\n",
      "Review 180000 of 219285\n",
      "Review 181000 of 219285\n",
      "Review 182000 of 219285\n",
      "Review 183000 of 219285\n",
      "Review 184000 of 219285\n",
      "Review 185000 of 219285\n",
      "Review 186000 of 219285\n",
      "Review 187000 of 219285\n",
      "Review 188000 of 219285\n",
      "Review 189000 of 219285\n",
      "Review 190000 of 219285\n",
      "Review 191000 of 219285\n",
      "Review 192000 of 219285\n",
      "Review 193000 of 219285\n",
      "Review 194000 of 219285\n",
      "Review 195000 of 219285\n",
      "Review 196000 of 219285\n",
      "Review 197000 of 219285\n",
      "Review 198000 of 219285\n",
      "Review 199000 of 219285\n",
      "Review 200000 of 219285\n",
      "Review 201000 of 219285\n",
      "Review 202000 of 219285\n",
      "Review 203000 of 219285\n",
      "Review 204000 of 219285\n",
      "Review 205000 of 219285\n",
      "Review 206000 of 219285\n",
      "Review 207000 of 219285\n",
      "Review 208000 of 219285\n",
      "Review 209000 of 219285\n",
      "Review 210000 of 219285\n",
      "Review 211000 of 219285\n",
      "Review 212000 of 219285\n",
      "Review 213000 of 219285\n",
      "Review 214000 of 219285\n",
      "Review 215000 of 219285\n",
      "Review 216000 of 219285\n",
      "Review 217000 of 219285\n",
      "Review 218000 of 219285\n",
      "Review 219000 of 219285\n",
      "Wall time: 14min 4s\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in df_data[\"review_text\"]:\n",
    "    clean_train_reviews.append( lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem( review))\n",
    "\n",
    "%time trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b = np.array(df_data.review_stars.copy())\n",
    "# b[(b == 1) | (b == 2) | (b == 3)] = 0\n",
    "# b[(b == 4) | (b == 5)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unique, counts = np.unique(b, return_counts=True)\n",
    "\n",
    "# print np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainDataVecs = np.column_stack((trainDataVecs, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDataVecs = np.column_stack((trainDataVecs, df_data.pos_words))\n",
    "trainDataVecs = np.column_stack((trainDataVecs, df_data.neg_words))\n",
    "trainDataVecs = np.column_stack((trainDataVecs, df_data.net_senti))\n",
    "trainDataVecs = np.column_stack((trainDataVecs, df_data.review_senti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219285L, 104L)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_add_feature_matrix_file = os.path.join(YELP_DATA_WORD_2_VEC_MODEL_DIR_FEAT_ADD, \"word2vec_add_feature_matrix_10Percent.csv\")\n",
    "np.savetxt(word2vec_add_feature_matrix_file, trainDataVecs, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.genfromtxt(\n",
    "    word2vec_add_feature_matrix_file,           # file name\n",
    "    delimiter=',')           # column delimiter                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(trainDataVecs, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
