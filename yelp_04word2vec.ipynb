{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEED_VAL = 200;\n",
    "WORK_DIR = os.getcwd();\n",
    "data_subset = \"_10Percent\"\n",
    "YELP_DATA_CSV_DIR = os.path.join(WORK_DIR, \"data\", \"csv\")\n",
    "YELP_DATA_WORD_2_VEC_MODEL_DIR = os.path.join(WORK_DIR, \"data\", \"word2vec_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_sure_path_exists(YELP_DATA_WORD_2_VEC_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2723: DtypeWarning: Columns (5,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "read_filename = os.path.join(YELP_DATA_CSV_DIR, 'business_review_user' + data_subset + '.csv')\n",
    "df_data = pd.read_csv(read_filename, engine='c', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Great outdoor patio dining area. Great happy hour. Great service.\\r\\n\\r\\nOutdoor patio dining has a beautiful mesquite tree for ambiance and blocking out the sun while the center fireplace keeps you warm. \\r\\n\\r\\nWe had:\\r\\nQueso Skillet with warm tortilla chips - amazing!\\r\\nMac N Cheese with Chorizo - fabulous! one of the best mac n cheeses I've ever had!\\r\\nCarne Asada on a Potato Pancake - was ok. Sounded excellent, tasted decent.\\r\\n\\r\\nFriendly and good food. But the ambiance really puts it over the top as a great dining experience. I'd be back with a group of friends to lounge, play cornsack or bocce ball during happy hour.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.review_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem(s):\n",
    "    s = remove_numbers_in_string(s)\n",
    "    s = lowercase_remove_punctuation(s)\n",
    "    s = remove_stopwords(s)\n",
    "    token_list = nltk.word_tokenize(s)\n",
    "    #token_list = filter_out_more_stopwords(token_list)\n",
    "    token_list = stem_token_list(token_list)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'great', u'outdoor', u'patio', u'dine', u'area', u'great', u'happi', u'hour', u'great', u'servic', u'outdoor', u'patio', u'dine', u'beauti', u'mesquit', u'tree', u'ambianc', u'block', u'sun', u'center', u'fireplac', u'keep', u'warm', u'queso', u'skillet', u'warm', u'tortilla', u'chip', u'amaz', u'mac', u'n', u'chees', u'chorizo', u'fabul', u'one', u'best', u'mac', u'n', u'chees', u'ive', u'ever', u'carn', u'asada', u'potato', u'pancak', u'ok', u'sound', u'excel', u'tast', u'decent', u'friendli', u'good', u'food', u'ambianc', u'realli', u'put', u'top', u'great', u'dine', u'experi', u'id', u'back', u'group', u'friend', u'loung', u'play', u'cornsack', u'bocc', u'ball', u'happi', u'hour']\n"
     ]
    }
   ],
   "source": [
    "print lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem(df_data.review_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem( raw_sentence))\n",
    "    \n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print \"Parsing sentences from training set\"\n",
    "for review in df_data[\"review_text\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'locat', u'chevron', u'ga', u'station', u'lot']\n"
     ]
    }
   ],
   "source": [
    "print sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 965M (CNMeM is disabled, CuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import gensim, logging\n",
    "from gensim.models import Word2Vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing model\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 8       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model_file = os.path.join(YELP_DATA_WORD_2_VEC_MODEL_DIR, str(num_features) + 'features_' + str(min_word_count) + 'minwords_' + str(context) + 'context'+ data_subset)\n",
    "if not os.path.isfile(model_file):\n",
    "    print \"Training model...\"\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "    model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "                size=num_features, min_count = min_word_count, \\\n",
    "                window = context, sample = downsampling)\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "    model.init_sims(replace=True)\n",
    "    model.save(model_file)\n",
    "else:\n",
    "    print \"Loading existing model\"\n",
    "    model = Word2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amazing'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"amazing delightful bad\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000. == 0.:\n",
    "           print \"Review %d of %d\" % (counter, len(reviews))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 219285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:44: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 219285\n",
      "Review 2000 of 219285\n",
      "Review 3000 of 219285\n",
      "Review 4000 of 219285\n",
      "Review 5000 of 219285\n",
      "Review 6000 of 219285\n",
      "Review 7000 of 219285\n",
      "Review 8000 of 219285\n",
      "Review 9000 of 219285\n",
      "Review 10000 of 219285\n",
      "Review 11000 of 219285\n",
      "Review 12000 of 219285\n",
      "Review 13000 of 219285\n",
      "Review 14000 of 219285\n",
      "Review 15000 of 219285\n",
      "Review 16000 of 219285\n",
      "Review 17000 of 219285\n",
      "Review 18000 of 219285\n",
      "Review 19000 of 219285\n",
      "Review 20000 of 219285\n",
      "Review 21000 of 219285\n",
      "Review 22000 of 219285\n",
      "Review 23000 of 219285\n",
      "Review 24000 of 219285\n",
      "Review 25000 of 219285\n",
      "Review 26000 of 219285\n",
      "Review 27000 of 219285\n",
      "Review 28000 of 219285\n",
      "Review 29000 of 219285\n",
      "Review 30000 of 219285\n",
      "Review 31000 of 219285\n",
      "Review 32000 of 219285\n",
      "Review 33000 of 219285\n",
      "Review 34000 of 219285\n",
      "Review 35000 of 219285\n",
      "Review 36000 of 219285\n",
      "Review 37000 of 219285\n",
      "Review 38000 of 219285\n",
      "Review 39000 of 219285\n",
      "Review 40000 of 219285\n",
      "Review 41000 of 219285\n",
      "Review 42000 of 219285\n",
      "Review 43000 of 219285\n",
      "Review 44000 of 219285\n",
      "Review 45000 of 219285\n",
      "Review 46000 of 219285\n",
      "Review 47000 of 219285\n",
      "Review 48000 of 219285\n",
      "Review 49000 of 219285\n",
      "Review 50000 of 219285\n",
      "Review 51000 of 219285\n",
      "Review 52000 of 219285\n",
      "Review 53000 of 219285\n",
      "Review 54000 of 219285\n",
      "Review 55000 of 219285\n",
      "Review 56000 of 219285\n",
      "Review 57000 of 219285\n",
      "Review 58000 of 219285\n",
      "Review 59000 of 219285\n",
      "Review 60000 of 219285\n",
      "Review 61000 of 219285\n",
      "Review 62000 of 219285\n",
      "Review 63000 of 219285\n",
      "Review 64000 of 219285\n",
      "Review 65000 of 219285\n",
      "Review 66000 of 219285\n",
      "Review 67000 of 219285\n",
      "Review 68000 of 219285\n",
      "Review 69000 of 219285\n",
      "Review 70000 of 219285\n",
      "Review 71000 of 219285\n",
      "Review 72000 of 219285\n",
      "Review 73000 of 219285\n",
      "Review 74000 of 219285\n",
      "Review 75000 of 219285\n",
      "Review 76000 of 219285\n",
      "Review 77000 of 219285\n",
      "Review 78000 of 219285\n",
      "Review 79000 of 219285\n",
      "Review 80000 of 219285\n",
      "Review 81000 of 219285\n",
      "Review 82000 of 219285\n",
      "Review 83000 of 219285\n",
      "Review 84000 of 219285\n",
      "Review 85000 of 219285\n",
      "Review 86000 of 219285\n",
      "Review 87000 of 219285\n",
      "Review 88000 of 219285\n",
      "Review 89000 of 219285\n",
      "Review 90000 of 219285\n",
      "Review 91000 of 219285\n",
      "Review 92000 of 219285\n",
      "Review 93000 of 219285\n",
      "Review 94000 of 219285\n",
      "Review 95000 of 219285\n",
      "Review 96000 of 219285\n",
      "Review 97000 of 219285\n",
      "Review 98000 of 219285\n",
      "Review 99000 of 219285\n",
      "Review 100000 of 219285\n",
      "Review 101000 of 219285\n",
      "Review 102000 of 219285\n",
      "Review 103000 of 219285\n",
      "Review 104000 of 219285\n",
      "Review 105000 of 219285\n",
      "Review 106000 of 219285\n",
      "Review 107000 of 219285\n",
      "Review 108000 of 219285\n",
      "Review 109000 of 219285\n",
      "Review 110000 of 219285\n",
      "Review 111000 of 219285\n",
      "Review 112000 of 219285\n",
      "Review 113000 of 219285\n",
      "Review 114000 of 219285\n",
      "Review 115000 of 219285\n",
      "Review 116000 of 219285\n",
      "Review 117000 of 219285\n",
      "Review 118000 of 219285\n",
      "Review 119000 of 219285\n",
      "Review 120000 of 219285\n",
      "Review 121000 of 219285\n",
      "Review 122000 of 219285\n",
      "Review 123000 of 219285\n",
      "Review 124000 of 219285\n",
      "Review 125000 of 219285\n",
      "Review 126000 of 219285\n",
      "Review 127000 of 219285\n",
      "Review 128000 of 219285\n",
      "Review 129000 of 219285\n",
      "Review 130000 of 219285\n",
      "Review 131000 of 219285\n",
      "Review 132000 of 219285\n",
      "Review 133000 of 219285\n",
      "Review 134000 of 219285\n",
      "Review 135000 of 219285\n",
      "Review 136000 of 219285\n",
      "Review 137000 of 219285\n",
      "Review 138000 of 219285\n",
      "Review 139000 of 219285\n",
      "Review 140000 of 219285\n",
      "Review 141000 of 219285\n",
      "Review 142000 of 219285\n",
      "Review 143000 of 219285\n",
      "Review 144000 of 219285\n",
      "Review 145000 of 219285\n",
      "Review 146000 of 219285\n",
      "Review 147000 of 219285\n",
      "Review 148000 of 219285\n",
      "Review 149000 of 219285\n",
      "Review 150000 of 219285\n",
      "Review 151000 of 219285\n",
      "Review 152000 of 219285\n",
      "Review 153000 of 219285\n",
      "Review 154000 of 219285\n",
      "Review 155000 of 219285\n",
      "Review 156000 of 219285\n",
      "Review 157000 of 219285\n",
      "Review 158000 of 219285\n",
      "Review 159000 of 219285\n",
      "Review 160000 of 219285\n",
      "Review 161000 of 219285\n",
      "Review 162000 of 219285\n",
      "Review 163000 of 219285\n",
      "Review 164000 of 219285\n",
      "Review 165000 of 219285\n",
      "Review 166000 of 219285\n",
      "Review 167000 of 219285\n",
      "Review 168000 of 219285\n",
      "Review 169000 of 219285\n",
      "Review 170000 of 219285\n",
      "Review 171000 of 219285\n",
      "Review 172000 of 219285\n",
      "Review 173000 of 219285\n",
      "Review 174000 of 219285\n",
      "Review 175000 of 219285\n",
      "Review 176000 of 219285\n",
      "Review 177000 of 219285\n",
      "Review 178000 of 219285\n",
      "Review 179000 of 219285\n",
      "Review 180000 of 219285\n",
      "Review 181000 of 219285\n",
      "Review 182000 of 219285\n",
      "Review 183000 of 219285\n",
      "Review 184000 of 219285\n",
      "Review 185000 of 219285\n",
      "Review 186000 of 219285\n",
      "Review 187000 of 219285\n",
      "Review 188000 of 219285\n",
      "Review 189000 of 219285\n",
      "Review 190000 of 219285\n",
      "Review 191000 of 219285\n",
      "Review 192000 of 219285\n",
      "Review 193000 of 219285\n",
      "Review 194000 of 219285\n",
      "Review 195000 of 219285\n",
      "Review 196000 of 219285\n",
      "Review 197000 of 219285\n",
      "Review 198000 of 219285\n",
      "Review 199000 of 219285\n",
      "Review 200000 of 219285\n",
      "Review 201000 of 219285\n",
      "Review 202000 of 219285\n",
      "Review 203000 of 219285\n",
      "Review 204000 of 219285\n",
      "Review 205000 of 219285\n",
      "Review 206000 of 219285\n",
      "Review 207000 of 219285\n",
      "Review 208000 of 219285\n",
      "Review 209000 of 219285\n",
      "Review 210000 of 219285\n",
      "Review 211000 of 219285\n",
      "Review 212000 of 219285\n",
      "Review 213000 of 219285\n",
      "Review 214000 of 219285\n",
      "Review 215000 of 219285\n",
      "Review 216000 of 219285\n",
      "Review 217000 of 219285\n",
      "Review 218000 of 219285\n",
      "Review 219000 of 219285\n",
      "Wall time: 4min\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_train_reviews = []\n",
    "for review in df_data[\"review_text\"]:\n",
    "    clean_train_reviews.append( lowercase_remove_punctuation_and_numbers_and_tokenize_and_filter_more_stopwords_and_stem( review))\n",
    "\n",
    "%time trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# b = np.array(df_data.review_stars.copy())\n",
    "# b[(b == 1) | (b == 2) | (b == 3)] = 0\n",
    "# b[(b == 4) | (b == 5)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  706]\n",
      " [   1 1519]]\n"
     ]
    }
   ],
   "source": [
    "# unique, counts = np.unique(b, return_counts=True)\n",
    "\n",
    "# print np.asarray((unique, counts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trainDataVecs = np.column_stack((trainDataVecs, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01919049,  0.00020651,  0.07834566, ...,  0.02025122,\n",
       "         0.01151156, -0.01196238],\n",
       "       [ 0.01497182,  0.00740789,  0.00292345, ...,  0.00717593,\n",
       "         0.01075724, -0.00030977],\n",
       "       [-0.00480012,  0.00025306,  0.00292017, ..., -0.01008894,\n",
       "        -0.00847206, -0.00038908],\n",
       "       ..., \n",
       "       [ 0.02289628, -0.01005408,  0.00365694, ...,  0.0110136 ,\n",
       "         0.00381733, -0.01780435],\n",
       "       [-0.01625548,  0.01062582, -0.00720449, ...,  0.0120312 ,\n",
       "         0.02036202, -0.00255128],\n",
       "       [-0.01710371, -0.00243994,  0.02431394, ...,  0.01426501,\n",
       "         0.01236991, -0.00726273]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219285L, 300L)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_feature_matrix_file = os.path.join(YELP_DATA_WORD_2_VEC_MODEL_DIR, 'word2vec_feature_matrix'+data_subset+'.csv')\n",
    "np.savetxt(word2vec_feature_matrix_file, trainDataVecs, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.genfromtxt(\n",
    "    word2vec_feature_matrix_file,           # file name\n",
    "    delimiter=',')           # column delimiter                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_equal(trainDataVecs, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
